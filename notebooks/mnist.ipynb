{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wF5wszaj97Y"
   },
   "source": [
    "# Introduction to Deep Learning with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you understand the fundamentals of deep learning, this demo will walk through the basic steps of building a toy model for classifying handwritten numbers with accuracies surpassing 95%. This model will be a basic fully-connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04QgGZc9bF5D"
   },
   "source": [
    "## The Task for the AI\n",
    "\n",
    "Our goal is to construct and train an artificial neural network on thousands of images of handwritten digits so that it may successfully identify others when presented. The data that will be incorporated is the MNIST database which contains 60,000 images for training and 10,000 test images. We will use the Keras Python API with TensorFlow as the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nnrWf3PCEzXL"
   },
   "source": [
    "![MNIST](https://www.researchgate.net/profile/Kofi_Appiah/publication/252028600/figure/fig2/AS:298067136925718@1448076151592/Some-samples-of-the-MNIST-database.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0trJmd6DjqBZ"
   },
   "source": [
    "## Prerequisite Python Modules\n",
    "\n",
    "First, some software needs to be loaded into the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "7NAbSZiaoJ4z"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow as tf\n",
    "    if tf.__version__.startswith('2.4'):\n",
    "        pass\n",
    "    else:\n",
    "        !pip3 install tensorflow-gpu==2.4.0\n",
    "except:\n",
    "    !pip3 install tensorflow-gpu==2.4.0\n",
    "    \n",
    "import tensorflow as tf\n",
    "import numpy as np                   # advanced math library\n",
    "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
    "import random                        # for generating random numbers\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.datasets import mnist     # MNIST dataset is included in Keras\n",
    "from tensorflow.keras.models import Sequential  # Model type to be used\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation # Types of layers to be used in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7FP5258xjs-v"
   },
   "source": [
    "## Loading Training Data\n",
    "\n",
    "The MNIST dataset is conveniently bundled within Keras, and we can easily analyze some of its features in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "BPZ68wASog_I"
   },
   "outputs": [],
   "source": [
    "# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3IKyzTCDNGo"
   },
   "source": [
    "Using matplotlib, we can plot some sample images from the training set directly into this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "ix4mEL65on-w"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    num = random.randint(0, len(X_train))\n",
    "    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[num]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7dTAzgHDUh7"
   },
   "source": [
    "Let's examine a single digit a little closer, and print out the array representing the last digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "T4JfEh7kvx6m"
   },
   "outputs": [],
   "source": [
    "# just a little function for pretty printing a matrix\n",
    "def matprint(mat, fmt=\"g\"):\n",
    "    col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n",
    "    for x in mat:\n",
    "        for i, y in enumerate(x):\n",
    "            print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n",
    "        print(\"\")\n",
    "\n",
    "# now print!        \n",
    "matprint(X_train[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pixel is an 8-bit integer from 0-255. 0 is full black, while 255 is full white. This what we call a single-channel pixel. It's called monochrome.\n",
    "\n",
    "*Fun-fact! Your computer screen has three channels for each pixel: red, green, blue. Each of these channels also likely takes an 8-bit integer. 3 channels -- 24 bits total -- 16,777,216 possible colors!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the input data layer\n",
    "\n",
    "Instead of a 28 x 28 matrix, we build our network to accept a 784-length vector.\n",
    "\n",
    "Each image needs to be then reshaped (or flattened) into a vector. We'll also normalize the inputs to be in the range [0-1] rather than [0-255]. Normalizing inputs is generally recommended, so that any additional dimensions (for other network architectures) are of the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flatten](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the input and test data\n",
    "Reshape the training and test data to a `60,000 x 784` and `10,000 x 784` matrix, cast it to `float32` and normalize it between `0` and `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then modify our classes (unique digits) to be in the one-hot format, i.e.\n",
    "\n",
    "```\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "etc.\n",
    "```\n",
    "\n",
    "If the final output of our network is very close to one of these classes, then it is most likely that class. For example, if the final output is:\n",
    "\n",
    "```\n",
    "[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0]\n",
    "```\n",
    "then it is most probable that the image is that of the digit `1`.\n",
    "\n",
    "Format `Y_train` and `Y_test` to a one hot encoding. Try using the `tensorflow.keras.utils.to_categorical` method for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a 3-layer fully connected network (FCN)\n",
    "![3layerMLP](https://image.slidesharecdn.com/prezentacja-dlworks-180401184922/95/czy-deep-learning-dziaa-29-638.jpg?cb=1522608727)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers and is very common.\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first hidden layer\n",
    "Add layers to the model using `model.add(...)`. See the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first hidden layer is a set of 512 nodes (artificial neurons).\n",
    "# Each node will receive an element from each input vector and apply some weight and bias to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An \"activation\" is a non-linear function applied to the output of the layer above.\n",
    "# It checks the new value of the node, and decides whether that artifical neuron has fired.\n",
    "# The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.\n",
    "# Those inputs are then not considered to be fired.\n",
    "# Positive values of a node are unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout zeroes a selection of random outputs (i.e., disables their activation)\n",
    "# Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second hidden layer appears identical to our first layer.\n",
    "# However, instead of each of the 512-node receiving 784-inputs from the input image data,\n",
    "# they receive 512 inputs from the output of the first 512-node layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final layer of 10 neurons in fully-connected to the previous 512-node layer.\n",
    "# The final layer of a FCN should be equal to the number of desired classes (10 in this case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"softmax\" activation represents a probability distribution over K different possible outcomes.\n",
    "# Its values are all non-negative and sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the built model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "Keras is built on top of Theano and TensorFlow. Both packages allow you to define a *computation graph* in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
    "\n",
    "When compiing a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical cross-entropy*, and is a loss function well-suited to comparing two probability distributions.\n",
    "\n",
    "Our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "The optimizer helps determine how quickly the model learns through **gradient descent**. The rate at which descends a gradient is called the **learning rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGD](data:image/jpeg;base64,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)\n",
    "![](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxIQEhUSEBAPFQ8VEhUQFhcWFRUYGBgQFRYWFhYTFhcZHSggGholGxMVJTEhJSkvLi4uFyA/ODMsQygtLisBCgoKDg0OFxAQGy4iHSUtKy0vNystMC4tLy43Mi0tKy0rKy03Ky0vNy04KzUvLS0vKzItLS0yLS0tLy0tOC01K//AABEIAJkBSQMBIgACEQEDEQH/xAAbAAEAAwEBAQEAAAAAAAAAAAAAAQUGBAIDB//EAEUQAAICAQEGAwMJBQYEBwEAAAECAAMEEQUGEhMhMUFRYRQicSMkMjNScoGhsUJigpGSNGN0g6KyFRZz0SVDRFSjweEH/8QAGAEBAQEBAQAAAAAAAAAAAAAAAAIBAwT/xAAsEQEBAAEBBQYFBQAAAAAAAAAAAQIRAxIxQVEEEyGhsdEyYXHw8SIjQpHB/9oADAMBAAIRAxEAPwD9xiIgIiICJk98sy1L8aupssK6ZLMuPy+NmrWsp9PpoCx/nODG3qaujHuuLWWewX328JCKbqWqR1ZSvRgzEE9NNG6GBu4lFuxtqzL53HXSEqsFSvVabEs9xXLKxReg4wOmvUGZ/B3remmivTm32DKtJsZwOXVeyBdVRjxHiAHTToesDexMhn758sUvyq+CxaGatrGF6C9wg+S4CBoT+0RrodJL713Cu64YqGqvLOEny3vWWjJ9nLacOirqde/cHp4wNdEyo29YzLXbWa7kz68VxXZxIQ9XNB4igLLoRqug6jvPhTvlZ7r2YoXHd8upCtvE5sxOaTqnCAFYUPp11106QNjEze628NmY9gNdArRKnD1WmwFrQTyz7igMoA1Gp7ieU3itIybDTjpRRa9CvZeV47EsCan3Dwro3qdRoB1BgaaJibN6cm1aGx6qOM5xw7Q1j8JARmBRjXrodAdSARp4zrxN8OZkCpadajkPicQLlhZWG4rGXg4RXxIRrxa9QdIGriIgIiICIni7i4Tw6cWh017cWnTX01ge4mD2ft+3Fps9p9qfPRaDZRaawrGy0VG3HdV0NZZ/w0XUDWdn/NeQjutuJWFpyacW5luJ65HL5bVgoOIDnLxa6eOmsDYRMsu85bIbGK06Fb+Fq7S5DUgahxwAA9ewY6ETk/8A57n23BebY767N2bb7xJ+UsrsLv8AEkDU+OkDaRMNjbztQgQ/KXW5ecFNjOFWqm9hoSqs3QFAABOjaG+hqpqu5NYD1Lc9T2MtwHFwsFrCHUDr7zFQYGxiZXJ3nuQZdgxVNOK5qB5ujWWfJnovDoo0s7k9x+M8ZW8No4q7quXdXl4VR5VvEDXk2oqniKDzYMunh0PXWBrYmWxt6LWtUNjKMdsy3BDizV+bWbNH4OH6J5ZHfXUzzu7vU+ZeK1qpFRqe0stxd04bAi12pwaK56nTi/ZPfSBq4mcG8FpfK+SoWjFY1myy7h4rOCtwSOE8KaWdST4dAZU5G92RZVrj14/OTaGPhvrY5rZLTWdUY169RYAdQNOpGviG5iZEb4scg0ijVEyExHKly/NZULOqhOHlqzgElgehOk10BERAREQEREBERARPi+UgcVl1FrKzquo4iqkBmA8QCy6/ET7QK3auxa8lq3c2q9XGEaux6yA4AYaoRqDwiVe190KbMflUAVstL0ISzkcFjpZZxHXiLMax7+vECdZpogUW7OyrcfmcxxwMVKViy21a9AeIiy33iWJ1I7DT1M9vuxRpWE51bVcwI9djq4W1uOxCwPVS2h0PkPKXUQKPL3Vx7W4n53UVBgLrAHNB4q2sAPvEHz7+Os6DsGjltVwnltkHLI4m+vN3P4tdddOZ107eEtIgVr7DpLmwqeM3pkk8R+urTlqdPLhHacG091arMc01e4y+0vUx1YJdkraHdlJ94fLv0PnNDEDN7rbDuxWbjcCooiLUtt1o41J1t47eoJBA4R06eM77936Hreoq3A93tJ0YhheLBaHVu4IdQR8JaxApP+V8fl8v5b6/2rj5tnM5+mnGH11106adupn3xthVV2G2trl4nNpQWvyzYw0Zimump7nw1695aRAREQESvydrImRVjaMbbUttGmmi11cAZm+JsUD4ywgJ4trDAqddCCDoSDoenQjqJ7nyryEZmRXUumnEoI1XiGq6jw1ECnTdPH0bj59hYVqWsusdglTixEUsei8SgkDv46zqv2FQ/MLKflbar395uttPByyOvTTlJ08dJZxApcXdjHrsFii3VebwqbXKLzyTbwoToNSdf0nRsnYlOL9SpGlNOP1Zj8lQGWsdT4Bj18ZZRAprN2qCAF5qMtttyuljq4e5i1mjA/RJb6J6dB5CfPN3TxrvrOeSaloci6wGytCzILDrq+hdj18zL2IFbZsOlktrKngvs5tnvHq/u9QfD6CyMnYNFjs7qSzvRa3vEe/jNxVHTw0P85ZxAq/+BUgABT7uQ+YvvHpkOXLN36jWxunbrKXdTdm/DdOK1eUlTVsq2XMLnJXhsKOeGrTRvdTX6WmugmuiBV3bCodLkZW4ciwW2e8wPMAQBlIOqkctO3iJzndXHNdlZ5x5lyZDObbDZz6+DgcOTqCOWvbp0l5ECpq2BUlptRr1LMHdRa4R7AoXjdNdCSFGvnp11ltEQERK/F2qtl92OqtxUrWXbpw8VgYhB114gACfRhAsIiICIiAiIgZnfihkrrzagTdh2c/Qa6tjkcORXoO+tZJA80WaLHuWxVdCCjKGUjsVI1B/kZ6dQQQQCCNCD4g+EzO5bGg3bPY9cV9atfHCt1anT0X3q/8ALgaiIiAiIgIiICIiAiJECYnNl51VQ1ttrQebMB+sq23nqbpj133nw5aHh/rbRfzmaxeOzyy4Rez5m5QeEsvFprpqNdPPSUhyM+36FWPQvnYxsb+hNAP6pnN9tjWLS2VdkNYaUZzoiodQPcrqCjXVnIHVvGTlleUdMNljr+vKTz9Pddbu/OM3My+6Iy7Op66jhp1a5h8bbGU/9ITUSn3S2V7HiU0Hq6oDYftXPq9jfi7MZby3CpmW2380zqMsdKr9MC/toGJJxrD8HLJ/mCaicG3tmLl49uO5IFiFdR3Vu6uD4EEAj4QLCJS7o7UbJxla0aZCFse8eWRUeB9PQkcQ9GEuYExIiBMSIgTEiIExIiBMSIgTEiIHm+0IpdjoqqWJ8lA1J/kJntwqicY5LjS3MtfMbUaEJZoKVPwqWsfhPG/1hfHTEQ6WZtyYY8xU2r3t+FKWdfMiaSqsKAqgBQAoHkB0Age4kRAmJEQJiIgJlt6/m12PtAa8Fbey5Gn/ALW4gBz9yzgbXwBaamc+0MNL6nptUNXYjVsD4qw0I/OB0CJndyM12obHubXJxLDiWk924ADVb/HWUb4kzRQET52Wqo1ZgB5kgCVFu9GMDw1ubX+zSrWHXy90ED8TMtkVjhllwi7kSiG0su36nD4B9q9wv+hOI/z0j/heVb9fmMo+zQgQfDibib9Jm90V3enxWTz9FvkZKVjV3RV82IA/OVT70UE6Vc25vKpGYf1fR/Oe6N2sZTxNVzH+1aTYf5uTp+EtUrA6AADyEeLf251vl7qX23Nt+rxq6l87n1P9Cf8AeSdj32fX5tun2aQKl+Go1b85dgT1G71O80+GSffzVGJu7jVniWlC/wBp9Xb+ptTLVVnqJskRllcuNRMxvN84ysTD7pxnOuH91jkctT8bmrP8BmnmX3THtF2VnHtZb7LT/hsYlNR9602t6jSalqIiICIiBlv7HtPyx89fwGdSv6vSP/h9ZqZS737LbJxXWogZKFcihvLJqPHXr6EjhPoxnVsDaq5mPVkICBYgYqe6v2ZG8irAg/CBYREQEREBERAREQEREBETm2lmpj1WXWHSutGsY+SqCT+kCgxz7TtR2/8AKwaBSvl7Vk6PZ+K1LWP80zUSg3Gw3rxFe4aZGQzZlwPcW3nj4P4VKr/DL+AiIgIiIExEQERED8+3s2nZs7OOVXTrVbRXRYSTwPYHblkkD3WXiI9Q3oJZYeRk5KhrsxKFbqFpQdVPb5WwEH8Jf7f2UmXj2479BYhXUd1burj1DAEfCV+5eXzsVVdFS6otj3IBoFvrOj6D7LfSHowkbt111envsJhJMZLOf5eqN2sU6M4a9u/Fc7W/yDHhH4CXNNKqNFVVA8AAB+U+D7PrJ1C8J80JU/6dJHs9q/QuJ9HUN+Y0M3SOVzuXGusCepxc+1fpVBh5ow/RtP1krtGvsxKH98FfzPSajSuyJ5RwRqCCPSeoYiIiaERECk3x2g9GK/K/tFhXGp8fl7jwIdPIa8R9FM79j7PTGoqor+hVWtY9eEaan1Pf8ZR5PzraaV96sGvnv5HLvBWofFa+YdP7xZqICIiAiIgJltj/ADPPuxe1GSGzqPIW6gZNY/iKv/G3lNTKDfLBd6RfSNcnFcZVQHdioPHV/GhZfxEC/icuzM5Miqu6pta7EWxT+6w1E6oCIiAiIgIiICIiAmY3yPPbGwB/6i3mW/4OjR7NfRm5afxmaeZjds+05WVm96w3sNB/u6Sea4+9aWH+WIGniIgIiICIiBMREBERATK5XzLaK2dsbO0ps8lza1+Sc9P20BXU+KJ5zVSs3j2SuZj2UE8JYaow7pcp4q7B6qwU/hAsolPuttVsrHVrBw5CE0Xr9nIrPC4+BPUHxDCW+sCZBUHvBM+b3ovd1HxIENk6Pk+z6ydQvC3mpKn/AEzz7PYv0LifR1DfmND+s+V23cVPpZNA+Ni/95zNvTiDtdxfcV3/ANoMzei5htLyv9O7n2r9KoN6ow/RtP1MkbSr/aJQ/vqV/M9D+Bld/wAy1n6FOY/wosH+4CBtm1ui4GUfvcpR+bzN6K7rLnP8XSuCNQQR6T5Z2YlFb22sFrrRrGJ8EUEk/wAhKBlyWOqYFVZPj7RwH4nlr1mP2420Mixtnn31tVrCrHUcutkLaOwBKcTINCNDqR5ybtNOVXs+zb/8pPrfy2m42I643PuBGTlWNmWg91NmnLrP3KxWv8M0U+OEritRYQbOEcWnbi066ems+06PPSIiGEREBERAy27fzPKuwD9U3Fm4v/Sdvlqh9yxtfhYPKamZ3fTDc1plUKTk4j+0Io7vXppdT/HXxAeoXyl1s7NS+pLqmDVWItiHzRhqD+cDoiIgIiICIiAiIgUu+G02xsV2q0OQ/Dj0DzyLSEr/AABbU+imdewtmriY9VCdVrrVNT3Yge8x9SdT+Mpbfne01XvRgJzG8jnXqQg+KUlj/nCamAiIgIiICIiBMREBERASDJkQPzvJwsrCzON8kpj5uTyzZWF1W4rpRxhgQAQvBr5hPOac7vsfp5ua38ap/tUTu23suvLosos14LF4dR3Vu6up8GUgEHzE4d0dqPfU1eRoMzHf2e8DsbFAK2r+66lWH3tPCRMJHfLtGd6T6SJXdej9s5D/AHr7T+XFPom6+GOvstJP7y8X6y4ibuxF2ufWuOnZlCfRopHwRR/9TqVAOwA/Ceom6IuVvE0iImsQdB8O8zG6A9psv2g3a9uTRr4YVJIVh99y76+IK+U+m+uS7Vph0sRflvyQR3SgDW+700TUA/adZf4mOtSLXWAtaKEUDsFUaAfyED6iIiAiIgIiICIiB4uUlSAdCQQD5HzmM3J5mFa+z8gjXh59LD6LKSeYq+WhOun7xm2mb33qValyAwS+hxZW2h017NWxHZWGo1MjKc+jvsctddn19eTSRKjdnbIzauaE4RxsgGuvQadfzlvKl1mscs8bhlccuMIiJqSIiAnHtjaKYtFl9mvBWhc6dzoOigeJJ0AHrOyZbbh9szKcMdaaeHOyfIlW+bVH4upf4VDzgdu52zXoxgbv7Vc7ZV5/v7TxFdfJBwoPRBLyIgIiICIiAiIgTERAREQEgyZBgJlt5kOHeu0qweWFFGYo/axddVu9TUST91m9JqZ5sQMCCAVIIIPYg9wYCtwwDKQVIBBHYg9QRPUyewLDs+//AIfaT7O+r4Tn7A6tiE/aTuvmn3TNZAREQEgnTv27yZmN78lrmr2dQSLckE3MverBX619R2ZtQi+rE/smB53W+d3W7Rb6t/m2L/hEPvWjy5jgn7qpNTPljULWioihUVQiqOwVRoAPwn1gIiICIiAiIgIiICeXUEaEaieogYnZFVuzslluZfZMixhWV6KlpOqhhp7uo6eWom1BnFtnATIpsqdeIMpGmuh4v2SD4EHTQzH7nbbyEyPZM9mForWtAdDxMpY8Wo8ShH9M56zDScnsuOXaJlnOMnj85192+iInR4yIiBybX2imLTZfadK60LnzOngB4knQAesrNz9nPXU114+d5LnJu/dLABKvgiBV/A+c4tof+IZq4464mGy339tHy/pU0fwdLD68HrNXAREQEREBERAREQJiIgIiICQZMgwERECu2/sdMyk1WFlOodHXo9dynVLUPgwP/wC95X7ubbdnOHmBUz611OnRL6h0GRT6H9pe6np5E6GVe39h15iAOWS1G46rUOllVn20b9QehHQgwLSJla9u5OH7m0KHsrHQZWOjOjDpobaV1etvPQFencT6279YOmlVrX2noKqUd7CfLhA934toBAtdubWrw6Wut10Giqo6s9jHRK0HizEgAes4N1Nk2VizIytDnZJFluh1FaD6vGQ/ZQH8WLHxnw2Xsm7IuXMz1Cumvs+ODxLQD0Njns95HQnso6DxJ00BERAREQEREBERAREQEREBM5vbu/z159LCvNp0srs8+HX5N/NSCR6a/hNHBmWarwzuF1jKbnb2+3EoUCutYZuuvv8AEVYD07EH1mrlDtnd7mMLsVxj5ia8NgXVWB713J040OnxHgZy1b2NR7m0ca3HftzEVrcdvVbEBK/BwD8ZmMsnira5YZZa4TSNRKPenbL0KtWOA+feTXQh7Bv2rrPKtAdSfgO5E5L99KH9zBV8y89FWoHgB87LSOFFHqdfIGdW7+xGqZ8nJdbM60AOwHu11jqtFQPZB592PUynJ1bu7HXDoWlWLNqz2O30rLnPFZa3qzEn06DwlnEQEREBERAREQERECYiICIiAkGTIMBERAREQEgKB4DWTEBERAREQEREBERAREQEREBERAREQEjSTECAoHYCTEQEREBERAREQEREBERA/9k=)\n",
    "So are smaller learning rates better? Not quite! It's important for an optimizer not to get stuck in local minima while neglecting the global minimum of the loss function. Sometimes that means trying a larger learning rate to jump out of a local minimum.\n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/r/robonchu/20171216/20171216162938.png)\n",
    "\n",
    "Compile the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the Adam optimizer for learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "This is the fun part! \n",
    "\n",
    "The batch size determines over how much data per step is used to compute the loss function, gradients, and back propagation. Large batch sizes allow the network to complete it's training faster; however, there are other factors beyond training speed to consider.\n",
    "\n",
    "Too large of a batch size smoothes the local minima of the loss function, causing the optimizer to settle in one because it thinks it found the global minimum.\n",
    "\n",
    "Too small of a batch size creates a very noisy loss function, and the optimizer may never find the global minimum.\n",
    "\n",
    "So a good batch size may take some trial and error to find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model.fit() function here. We recommend setting verbose=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two numbers, in order, represent the value of the loss function of the network on the training set, and the overall accuracy of the network on the training data. But how does it do on data it did not train on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model's Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the output\n",
    "\n",
    "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    \n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rX8mhOLljYeM"
   ],
   "name": "beginner.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
